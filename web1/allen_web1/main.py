#!/usr/bin/env python
# coding: utf-8

# In[4]:


#!/usr/bin/env python
# coding: utf-8
from monitor import monitor
import sys
import traceback
import os
from selenium import webdriver
import time
import requests
from bs4 import BeautifulSoup as bs
from selenium.webdriver.chrome.options import Options
from tqdm import tqdm
import pandas as pd
from sqlalchemy import create_engine
from datetime import datetime 
import sqlalchemy
#logè®Šæ•¸
#-------------------------------------------------------------
monitor = monitor() #å¼•å…¥ç›£æ§ç¨‹å¼
daytime = monitor.daytime() #æ™‚é–“
absFilePath = os.path.abspath('') #è·¯å¾‘ï¼ˆåˆ‡æ›æˆ.pyä½¿ç”¨__file__ï¼‰
path, filename = os.path.split(absFilePath) #è·¯å¾‘
confini = "config.ini" # configåç¨±ï¼ˆconfigé ˆè‡ªè¡Œå»ºç«‹ï¼‰
#=============================================================

#è¨ˆæ™‚é–‹å§‹
#-------------------------------------------------------------
timeS = monitor.timing()
#=============================================================

#confè®Šæ•¸
#-------------------------------------------------------------
errlogfilename, logfilename, serialnumfile ,codenum , to_mail , gmail_user, gmail_password , Subject , logdb ,logtable , loguser, logpw , logip , setlogfile ,token= monitor.conf(confini)
#=============================================================
#ä¸»ç¨‹å¼
#-------------------------------------------------------------
# your function
def UrlAndCrawl():
    try:
        page_list = ["career","education","living","health","senior","pets","language","design","creative-industry","technology","business","investment-finance"]
        options = webdriver.ChromeOptions()
        options.add_argument("--headless")
        driver = webdriver.Chrome(chrome_options = options, executable_path = "/Users/t3019ein/Desktop/chromedriver")
        driver.implicitly_wait(3)
        urls_list = []
        for x in page_list:
            for i in range(1,15):
                raw_page = driver.get(f"https://ggc.hkxf.com.tw/category/{x}/{i}")
                time.sleep(2)
                for j in range(1,11):
                    urls = driver.find_elements_by_xpath(f"//*[@id='CourseContainer']/div[{j}]/div[2]/a")
                    for k in urls:
                        href = k.get_attribute("href")
                        urls_list.append(href)

        item_list = []
        for z in urls_list:
            dic = {}
            driver.get(z)
            title = driver.find_elements_by_xpath("//div[@class='pr-2 pl-2'][2]/h2[@class='mt-3 mb-2 font-weight-bold']")
            driver.implicitly_wait(3)
        #           èª²ç¨‹æ¨™é¡Œ
            for a in title:
                dic["title"] = a.text.strip().replace("â­","").replace("ğŸ ","").replace("ã€Š","").replace("ã€‹","").replace("ã€","").replace("ã€‘","").replace("â€”","").replace("~","").replace("â”‚","").replace("Â®","")
            during = driver.find_elements_by_xpath("//ul[@class='course-info-list mt-20 mt-4 ml-0 pl-0']/li[1]/span[@class='course-info-details text-gray-darkgray']")
            driver.implicitly_wait(3)
        #           èª²ç¨‹æ™‚ç¨‹
            for c in during:
                dic["during"] = c.text.strip()
            hours = driver.find_elements_by_xpath("//ul[@class='course-info-list mt-20 mt-4 ml-0 pl-0']/li[3]/span[@class='course-info-details text-gray-darkgray']")
            driver.implicitly_wait(3)
        #           èª²ç¨‹æ™‚æ•¸
            for d in hours:
                dic["hours"] = d.text.strip().replace("hr","")
            location = driver.find_elements_by_xpath("//ul[@class='course-info-list mt-20 mt-4 ml-0 pl-0']/li[5]/a/span[@class='course-info-details']")
            driver.implicitly_wait(3)
        # #           èª²ç¨‹åœ°é»
            for e in location:
                dic["location"] = e.text.strip()
            price = driver.find_elements_by_xpath("//ul[@class='course-info-list mt-20 mt-4 ml-0 pl-0']/li[6]/span[@class='course-info-details f-10 text-gray-darkgray']")
            driver.implicitly_wait(3)
        # #           èª²ç¨‹åƒ¹æ ¼
            for f in price:
                dic["price"] = f.text.strip().replace("NTD$","")
            coursetime = driver.find_elements_by_xpath("//ul[@class='course-info-list mt-20 mt-4 ml-0 pl-0']/li[2]/span[@class='course-info-details align-middle text-gray-darkgray']")
            driver.implicitly_wait(3)
#             èª²ç¨‹æ™‚é–“
            for g in coursetime:
                dic['time'] = g.text.strip()
            weekday = driver.find_elements_by_xpath("//ul[@class='course-info-list mt-20 mt-4 ml-0 pl-0']/li[1]/span[@class='course-info-details text-gray-darkgray']")
            driver.implicitly_wait(3)
#             èª²ç¨‹æ˜¯ç¦®æ‹œå¹¾
            for h in weekday:
                dic['weekday'] = h.text.strip()
            address = driver.find_elements_by_xpath("//ul[@class='course-info-list mt-20 mt-4 ml-0 pl-0']/li[5]/a/span[@class='course-info-details']")
            for v in address:
                dic['address'] = v.text.strip()
            today = datetime.today().strftime("%Y%m%d")
        # #           çˆ¬èŸ²æ™‚é–“
            dic["today"] = today
            item_list.append(dic)
    #         time.sleep(3)
    except NameError as e:
        print(str(e))
    finally:
        return item_list
        driver.quit()
def putintoSql(item_list):
#   å¯«é€²mysql
    df = pd.DataFrame(item_list)
    engine = create_engine('mysql+pymysql://allen:allen0319@192.168.56.150:3306/education?charset=utf8')
    df.to_sql('web1_test',engine,index=False,if_exists='replace')
    
#=============================================================

#å…ˆè¨­å®šéŒ¯èª¤å‚™è¨»ç‚ºç©ºå€¼
description = ''

#ä¸»è¦åŸ·è¡Œå€
#-------------------------------------------------------------
try:
    #logfileåˆ†éš”ç·š
    #--------------------------
    monitor.filewriteS(setlogfile)
    #--------------------------
    processnum = 1
    #function1
    description = 'åŸ·è¡Œå®Œç¬¬ä¸€æ­¥é©Ÿæ™‚éŒ¯èª¤'
    logtext = 'ä¸€èˆ¬logç´€éŒ„  ç¬¬ä¸€æ­¥'
    items = UrlAndCrawl()
    monitor.filewrite(setlogfile,logtext,processnum)
    #--------------------------
#     processnum = 2
#     #function2
#     item_list = main(urls_list)
#     description = 'åŸ·è¡Œå®Œç¬¬äºŒæ­¥é©Ÿæ™‚éŒ¯èª¤'
#     logtext = 'ä¸€èˆ¬logç´€éŒ„  ç¬¬äºŒæ­¥'
#     monitor.filewrite(setlogfile,logtext,processnum)
    #--------------------------
    processnum = 3
    #function3
    description = 'åŸ·è¡Œå®Œç¬¬ä¸‰æ­¥é©Ÿæ™‚éŒ¯èª¤'
    logtext = 'ä¸€èˆ¬logç´€éŒ„  ç¬¬ä¸‰æ­¥'
    putintoSql(items)
    monitor.filewrite(setlogfile,logtext,processnum)
    #--------------------------
    #.........
    state = "success"
    errnum = 0
except Exception as e:  
    state = "fail"
    errnum = 1
    #æ–‡ä»¶å¯«å…¥é»1 å¯«å…¥éŒ¯èª¤
    monitor.toErrFile(errlogfilename , daytime, filename, e ,processnum,description)
#=============================================================

# æ¯”å°  æ‡‰æ”¾å…¥è³‡æ–™åº«ç­†æ•¸ / å·²å…¥è³‡æ–™åº«ç­†æ•¸
# note:æ­¤è™•å› ç‚ºæ²’æœ‰çœŸå¯¦è³‡æ–™å…ˆä½¿ç”¨logè³‡æ–™ï¼Œä¸Šç·šå¾Œè«‹æ”¹æˆçˆ¬èŸ²æˆ–æ¸…æ´—è³‡æ–™
#------------------------------------------------------------- 
try:
    #è«‹æ”¾å…¥æ­¤æ¬¡åŸ·è¡Œæ‡‰é€²SQLè¡Œæ•¸ï¼ˆæ”¹æˆdfè¡Œæ•¸ï¼‰
    sSQL = len(item_list)
    #è«‹selectå‡ºæ­¤æ¬¡åŸ·è¡Œå·²é€²å…¥SQLè¡Œæ•¸(æ”¹æˆå·²è¼¸å…¥çš„DATAæ•¸é‡)
    sql = "select count(*) as cou from web1_test;"
    #SQL query è«‹æ›´æ”¹ ip account password databases query
    db, cursor ,accounts = monitor.pymysqlcon(logip, loguser, logpw, logdb ,sql)
    aSQL = accounts[0]['cou']
except:
    sSQL = 0
    aSQL = 99
#=============================================================  

#åŸ·è¡Œæ¬¡æ•¸ç´€éŒ„
#-------------------------------------------------------------   
monitor.serialnum(serialnumfile,codenum,errnum) 
#=============================================================  

#èŠ±è²»æ™‚é–“
#-------------------------------------------------------------   
timeE = monitor.timing() 
timeSP = timeE -timeS
#=============================================================   

#æ–‡ä»¶å¯«å…¥é»2 å¯«å…¥logæª”
#-------------------------------------------------------------   
monitor.toFile(logfilename , daytime, timeS, timeE ,filename ,state ,sSQL ,aSQL )
#=============================================================   

# log to SQL
#-------------------------------------------------------------

missSQL = sSQL - aSQL  #æ¼æ‰è³‡æ–™é‡
try:
    #å»ºç«‹ SQL èªæ³• insert & createtable
    sql , sqlcreate = monitor.logSQL(logtable, daytime,timeSP,filename,state,sSQL,aSQL ,missSQL ,logdb)
    #log to SQL 
    #  1.create table
    monitor.pymysqlcon(logip, loguser, logpw, logdb ,sqlcreate)
    #  2.insert log
    monitor.pymysqlcon(logip, loguser, logpw, logdb ,sql)
    logerrnum =0
except:
    logerrnum = 1
#=============================================================  

# line Send error message
#-------------------------------------------------------------   
if errnum == 1:        
    # ä¿®æ”¹ç‚ºä½ è¦å‚³é€çš„è¨Šæ¯å…§å®¹
    message = str(errlogfilename) +"\n"+ str(daytime) +"\n"+ str(filename) +"\n"+ str(processnum) +"\n"+ str(description)
    # ä¿®æ”¹ç‚ºä½ çš„æ¬Šæ–å…§å®¹
    monitor.lineNotifyMessage(token, message)
    
if logerrnum == 1:        
    # ä¿®æ”¹ç‚ºä½ è¦å‚³é€çš„è¨Šæ¯å…§å®¹
    message = str(errlogfilename) +"\n"+ str(daytime) +"\n"+ str(filename) +"\n"+ 'log to SQL error'
    # ä¿®æ”¹ç‚ºä½ çš„æ¬Šæ–å…§å®¹
    monitor.lineNotifyMessage(token, message)
#=============================================================  

# ä¸€ã€è‡ªå®šç¾©log å»ºè­°
# --- START crawing at 2020-05-13 11:42:20.386133 ---
# ---
# Finished crawing [ spark ] at 2020-05-13 11:44:05.156622
# [Success] Check Point 1 : CorpNo. 71 = JobNo. 71
# [Success] Check Point 2 : CorpNo. and JobNo. (71/71) = TotalJobs 71 and NO Exceptions
# [Success] Check Point 3 : CorpNo. or JobNo. (71/71) = InsertedJobs 71 
# ---

# äºŒã€lineå‚³é€éŒ¯èª¤è¨Šæ¯
# å» https://notify-bot.line.me/zh_TW/ å€‹äººé é¢è¨­å®š tocken


# In[ ]:




